{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Task 2**\n",
    "\n",
    "---\n",
    "\n",
    "## **Buidling a Machine Learning Model**\n",
    "\n",
    "This task involves choosing an appropriate algorithm, training the model, and evaluating its performance. The goal is to create a model that can be easily understood and acted on by business stakeholders. \n",
    "\n",
    "The challenge lies in selecting the right machine learning algorithm and fine-tuning it to accurately predict which customers are at risk of leaving. This model will provide actionable insights, enabling the team to develop targeted interventions to retain valuable customers. Specifically, it is important to select the most appropriate machine learning algorithm, which balances predictive accuracy with interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exploratory data analysis**\n",
    "\n",
    "First, we must load all sheets in order to better understand what we have and the statistical properties of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import data preprocessing libraries\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Ignore Warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Excel file\n",
    "file_path = \"Data/Customer_Churn_Data_Large.xlsx\"\n",
    "xlsx = pd.ExcelFile(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sheets into separate DataFrames\n",
    "demographics = xlsx.parse(\"Customer_Demographics\")\n",
    "transactions = xlsx.parse(\"Transaction_History\")\n",
    "service = xlsx.parse(\"Customer_Service\")\n",
    "online = xlsx.parse(\"Online_Activity\")\n",
    "churn = xlsx.parse(\"Churn_Status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start merging all the sheets by `CustomerID` column. To tackle the problem of **transactional format** of `Transaction_History` and `Customer_Service` sheets, I will aggregate the transactional data first using `.groupby('CustomerID')` and applied functions like _sum_, _count_, and _mean_.\n",
    "\n",
    "This turned the transactional sheets into **customer-level summaries** (i.e., each row = one customer), making it possible to:\n",
    "\n",
    "- Merge them safely with `Customer_Demographics`, `Online_Activity`, and `Churn_Status`.\n",
    "\n",
    "- Do EDA with one observation per customer.\n",
    "\n",
    "- Fit a proper model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if missing values still exist in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Preprocessing** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building any predictive model, it’s critical to ensure that the dataset is clean, balanced, and properly preprocessed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Handle Missing Values**\n",
    "\n",
    "- `ServiceInteractions` and `ResolutionRate` have 332 missing values — these likely came from customers who had no customer service records.\n",
    "\n",
    "    → Treat missing values as **meaningful absence**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing service-related fields with 0 (no interaction)\n",
    "base_df['ServiceInteractions'] = base_df['ServiceInteractions'].fillna(0)\n",
    "base_df['ResolutionRate'] = base_df['ResolutionRate'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Feature Engineering**\n",
    "\n",
    "- Convert `LastLoginDate` to days since last login event and drop the column.\n",
    "\n",
    "- Create new feature `AgeGroup` based on `Age` for better customer segmentation purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert LastLoginDate to days since last login\n",
    "base_df['LastLoginDate'] = pd.to_datetime(base_df['LastLoginDate'])\n",
    "ref_date = base_df['LastLoginDate'].max()\n",
    "base_df['DaysSinceLastLogin'] = (ref_date - base_df['LastLoginDate']).dt.days\n",
    "base_df.drop(columns=['LastLoginDate'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create age bins\n",
    "base_df['AgeGroup'] = pd.cut(base_df['Age'], bins=[0, 30, 50, 100], labels=['<30', '30-50', '>50'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Outliers Detection and Treatment**\n",
    "\n",
    "- Use the `clip` method to limit the values in numeric features if there exist outliers. Given the interval, values outside the interval are clipped to the interval edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cap outliers at 1st and 99th percentile for numeric features\n",
    "for col in numeric_cols:\n",
    "    lower = base_df[col].quantile(0.01)\n",
    "    upper = base_df[col].quantile(0.99)\n",
    "    base_df[col] = base_df[col].clip(lower, upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Encoding and Scaling**\n",
    "\n",
    "- Use the **One-Hot Encoding** method to encode all the categorical features as it turns each category into a binary feature (dummy variable), avoiding any unintended ordinal interpretation.\n",
    "\n",
    "    - This method is ideal when categories don’t have a natural order like `Gender`, `MaritalStatus`, `AgeGroup` or `IncomeLevel`.\n",
    "\n",
    "- Scale numerical data to ensure all features contribute equally to the predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "data_for_export = base_df.drop(columns=[\"CustomerID\"])\n",
    "categorical = data_for_export.select_dtypes(include='object').columns.tolist()\n",
    "numerical = data_for_export.select_dtypes(include='number').drop(columns=['ChurnStatus']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical variables\n",
    "encoded_df = pd.get_dummies(data_for_export, columns=categorical, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "encoded_df[numerical] = scaler.fit_transform(encoded_df[numerical])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Export Processed Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export processed data for modeling task\n",
    "encoded_df.to_csv(\"Data/processed_churn_csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
